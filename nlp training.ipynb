{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN5u9h3asoo1Bdgy/1vIla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryanth003/assesment/blob/main/nlp%20training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Hello World\n",
        "print(\"Hello, World!\")\n",
        "\n",
        "# 2. Basic Arithmetic Operations\n",
        "a = 10\n",
        "b = 5\n",
        "print(f\"Addition: {a + b}\")\n",
        "print(f\"Subtraction: {a - b}\")\n",
        "print(f\"Multiplication: {a * b}\")\n",
        "print(f\"Division: {a / b}\")\n",
        "\n",
        "# 3. Variable Assignment and Printing\n",
        "name = \"Alice\"\n",
        "age = 30\n",
        "print(f\"My name is {name} and I am {age} years old.\")\n",
        "\n",
        "# 4. List Creation and Access\n",
        "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
        "print(f\"First fruit: {fruits[0]}\")\n",
        "print(f\"All fruits: {fruits}\")\n",
        "\n",
        "# 5. Loop through a List\n",
        "for fruit in fruits:\n",
        "    print(f\"I like {fruit}\")\n",
        "\n",
        "# 6. Conditional Statement (if-else)\n",
        "score = 75\n",
        "if score >= 60:\n",
        "    print(\"Passed!\")\n",
        "else:\n",
        "    print(\"Failed!\")\n",
        "\n",
        "# 7. Function Definition and Call\n",
        "def greet(name):\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "message = greet(\"Bob\")\n",
        "print(message)\n",
        "\n",
        "# 8. User Input\n",
        "user_name = input(\"Enter your name: \")\n",
        "print(f\"Nice to meet you, {user_name}!\")\n",
        "\n",
        "# 9. Dictionary Creation and Access\n",
        "person = {\"name\": \"Charlie\", \"age\": 25, \"city\": \"New York\"}\n",
        "print(f\"Person's name: {person['name']}\")\n",
        "print(f\"Person's age: {person.get('age')}\")\n",
        "\n",
        "# 10. String Manipulation\n",
        "sentence = \"Python is fun!\"\n",
        "print(f\"Uppercase: {sentence.upper()}\")\n",
        "print(f\"Replaced: {sentence.replace('fun', 'great')}\")\n",
        "\n",
        "# 11. Range Function and Loop\n",
        "for i in range(3):\n",
        "    print(f\"Count: {i}\")\n",
        "\n",
        "# 12. List Comprehension\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "squares = [x**2 for x in numbers]\n",
        "print(f\"Squares: {squares}\")\n",
        "\n",
        "# 13. While Loop\n",
        "count = 0\n",
        "while count < 3:\n",
        "    print(f\"While count: {count}\")\n",
        "    count += 1\n",
        "\n",
        "# 14. Basic Class Definition\n",
        "class Dog:\n",
        "    def __init__(self, name, breed):\n",
        "        self.name = name\n",
        "        self.breed = breed\n",
        "    def bark(self):\n",
        "        return f\"{self.name} says Woof!\"\n",
        "\n",
        "my_dog = Dog(\"Buddy\", \"Golden Retriever\")\n",
        "print(my_dog.bark())\n",
        "\n",
        "# 15. Tuple Creation and Access\n",
        "coordinates = (10, 20)\n",
        "print(f\"X coordinate: {coordinates[0]}\")\n",
        "\n",
        "# 16. Set Creation and Operations\n",
        "set1 = {1, 2, 3, 4}\n",
        "set2 = {3, 4, 5, 6}\n",
        "print(f\"Union: {set1.union(set2)}\")\n",
        "print(f\"Intersection: {set1.intersection(set2)}\")\n",
        "\n",
        "# 17. Error Handling (Try-Except)\n",
        "try:\n",
        "    result = 10 / 0\n",
        "except ZeroDivisionError:\n",
        "    print(\"Cannot divide by zero!\")\n",
        "\n",
        "# 18. Importing a Module (math)\n",
        "import math\n",
        "print(f\"Square root of 16: {math.sqrt(16)}\")\n",
        "print(f\"Pi: {math.pi}\")\n",
        "\n",
        "# 19. Simple File Write (creates 'my_file.txt' in the current directory)\n",
        "with open('my_file.txt', 'w') as f:\n",
        "    f.write('Hello from Python!\\n')\n",
        "print(\"File 'my_file.txt' created and written.\")\n",
        "\n",
        "# 20. Simple File Read (reads from 'my_file.txt' if it exists)\n",
        "try:\n",
        "    with open('my_file.txt', 'r') as f:\n",
        "        content = f.read()\n",
        "    print(f\"Content of 'my_file.txt':\\n{content}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'my_file.txt' not found. Run program 19 first.\")\n"
      ],
      "metadata": {
        "id": "wpQx2rlb1Gln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5El6TLFOzcwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086402e7-a5d3-4b03-9381-b84902f497ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "euG6t0hhMyPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "a=\"i love cars üòä\"\n",
        "tok=nltk.word_tokenize(a)\n",
        "print(tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tNPsNnplUXK",
        "outputId": "a6589342-dcba-49c8-ff71-625fc5f3fc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'cars', 'üòä']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Assuming 'tok' is available from previous cells\n",
        "normalized_tokens = [word.lower() for word in tok]\n",
        "normalized_tokens = [re.sub(r'[^a-z]', '', word) for word in normalized_tokens]\n",
        "normalized_tokens = [word for word in normalized_tokens if word]\n",
        "\n",
        "print(f\"Original tokens: {tok}\")\n",
        "print(f\"Normalized tokens (lowercase, alphanumeric only): {normalized_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU5ETuHi7Srg",
        "outputId": "40486d57-c718-4e4b-ee12-927f165d6bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['i', 'love', 'cars', 'üòä']\n",
            "Normalized tokens (lowercase, alphanumeric only): ['i', 'love', 'cars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nlt.download(stopwords)\n",
        "stop_words=set(stopwords.word('English'))\n"
      ],
      "metadata": {
        "id": "y7DTrSUN_JBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkqDtlF5AVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvUQgmXIAVdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cpcRKpNAVZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwJOsfQzAVWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwqpXZGhAWH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pASva27LAWC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6jI8zTlAV_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XChfVrt3AWxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Q9fp6ITIAWtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords # Import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "text = [\"this\", \"is\", \"a\", \"not \",\"a\", \"Car\"]\n",
        "res = [word for word in text if word.lower() not in stop_words]\n",
        "print(res)"
      ],
      "metadata": {
        "id": "kga4s1RCAWqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7891be00-7e31-4e61-d616-7e16bb082bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. Calculate the levenstain distance between START and STARE(matrics operation postword lemmetization)\n",
        "2. CLean a dirty sentence for a search engine(the input is:[\"The quick BROWN foxes...they are JUMPING over 10 lazy dogs!\"])\n",
        "3. BUild  a simple logic based on:\n",
        "- create a list of three spam words eg.,[\"will\",\"cash\",\"free\",\"prize\"] I/p: \"You are WINNING a free prize now\" o/p : in boolean format weather the spam word is present or not\""
      ],
      "metadata": {
        "id": "UlbTYBckAWm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fc20e0"
      },
      "source": [
        "### Task 1: Calculate Levenshtein Distance after Word Lemmatization\n",
        "\n",
        "First, we need to download the `wordnet` corpus for lemmatization if it hasn't been downloaded already. Then, we'll lemmatize the words \"START\" and \"STARE\" and calculate their Levenshtein distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "0e3a3198",
        "outputId": "a52a85e4-e713-4c21-c2c7-9c07fe118f61"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download wordnet corpus if not already present\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word1 = \"START\"\n",
        "word2 = \"STARE\"\n",
        "\n",
        "# Lemmatize words (assuming they are nouns for simplicity, 'v' for verb)\n",
        "lemmatized_word1 = lemmatizer.lemmatize(word1.lower(), pos='n')\n",
        "lemmatized_word2 = lemmatizer.lemmatize(word2.lower(), pos='n')\n",
        "\n",
        "# Calculate Levenshtein distance\n",
        "distance = nltk.edit_distance(lemmatized_word1, lemmatized_word2)\n",
        "\n",
        "print(f\"Original words: '{word1}', '{word2}'\")\n",
        "print(f\"Lemmatized words: '{lemmatized_word1}', '{lemmatized_word2}'\")\n",
        "print(f\"Levenshtein distance: {distance}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2644579711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2644579711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72eb535a"
      },
      "source": [
        "### Task 2: Clean a Dirty Sentence for a Search Engine\n",
        "\n",
        "This task involves lowercasing, removing punctuation, numbers, and stopwords from the given dirty sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "9dc96e69",
        "outputId": "16e4fbbf-d1e7-4fa5-cbc8-a4c9827c310f"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords and punkt if not already present\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "È´íÂè•Â≠ê = \"The quick BROWN foxes...they are JUMPING over 10 lazy dogs!\"\n",
        "\n",
        "# 1. Lowercase the sentence\n",
        "cleaned_sentence = È´íÂè•Â≠ê.lower()\n",
        "\n",
        "# 2. Remove punctuation and numbers\n",
        "cleaned_sentence = re.sub(r'[^a-z\\s]', '', cleaned_sentence) # Keep only letters and spaces\n",
        "\n",
        "# 3. Tokenize the sentence\n",
        "tokens = word_tokenize(cleaned_sentence)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "print(f\"Original sentence: '{È´íÂè•Â≠ê}'\")\n",
        "print(f\"Cleaned tokens: {filtered_tokens}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3327073038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3327073038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0671c6d5"
      },
      "source": [
        "### Task 3: Spam Word Detection Logic\n",
        "\n",
        "Here, we'll create a list of spam words and check if any of these words are present in a given input sentence. The output will be a boolean value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "8d45806b",
        "outputId": "708341fd-3e3d-4677-97b9-4d77b7c18f72"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download punkt if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "spam_words = [\"will\", \"cash\", \"free\", \"prize\"]\n",
        "input_sentence = \"You are WINNING a free prize now\"\n",
        "\n",
        "# Lowercase and tokenize the input sentence\n",
        "input_tokens = [word.lower() for word in word_tokenize(input_sentence)]\n",
        "\n",
        "# Check for presence of any spam word\n",
        "is_spam = any(word in input_tokens for word in spam_words)\n",
        "\n",
        "print(f\"Input sentence: '{input_sentence}'\")\n",
        "print(f\"Spam words list: {spam_words}\")\n",
        "print(f\"Is spam present? {is_spam}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1455227471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1455227471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcotGbS-AWkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QJcpVbiAVNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}